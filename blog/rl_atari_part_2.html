<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Reinforcement Learning via Atari Games, Part 2 - Chan Y. Park</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <link href="./favicon.ico" rel="icon">

<link rel="canonical" href="./rl_atari_part_2.html">

        <meta name="author" content="Chan Y. Park" />
        <meta name="keywords" content="machine learning,deep learning,reinforcement learning,tensorflow" />
        <meta name="description" content="This is the second blog post that will describe my project of implementing reinforcement learning of Atari games using TensorFlow and OpenAI Gym." />

        <meta property="og:site_name" content="Chan Y. Park" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Reinforcement Learning via Atari Games, Part 2"/>
        <meta property="og:url" content="./rl_atari_part_2.html"/>
        <meta property="og:description" content="This is the second blog post that will describe my project of implementing reinforcement learning of Atari games using TensorFlow and OpenAI Gym."/>
        <meta property="article:published_time" content="2017-05-25" />
            <meta property="article:section" content="Deep Learning" />
            <meta property="article:tag" content="machine learning" />
            <meta property="article:tag" content="deep learning" />
            <meta property="article:tag" content="reinforcement learning" />
            <meta property="article:tag" content="tensorflow" />
            <meta property="article:author" content="Chan Y. Park" />


    <!-- Bootstrap -->
        <link rel="stylesheet" href="./theme/css/bootstrap.min.css" type="text/css"/>
    <link href="./theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="./theme/css/pygments/default.css" rel="stylesheet">
    <link rel="stylesheet" href="./theme/css/style.css" type="text/css"/>
        <link href="./static/custom.css" rel="stylesheet">





</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="./" class="navbar-brand">
Chan Y. Park            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="./pages/about.html">
                             About Me
                          </a></li>
                        <li class="active">
                            <a href="./category/deep-learning.html">Deep learning</a>
                        </li>
                        <li >
                            <a href="./category/experiences.html">Experiences</a>
                        </li>
                        <li >
                            <a href="./category/python.html">Python</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="./rl_atari_part_2.html"
                       rel="bookmark"
                       title="Permalink to Reinforcement Learning via Atari Games, Part 2">
                        Reinforcement Learning via Atari Games, Part 2
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-05-25T18:01:23.356556-04:00"> Thu 25 May 2017</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="./tag/machine-learning.html">machine learning</a>
        /
	<a href="./tag/deep-learning.html">deep learning</a>
        /
	<a href="./tag/reinforcement-learning.html">reinforcement learning</a>
        /
	<a href="./tag/tensorflow.html">tensorflow</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="id10">Introduction</a></li>
<li><a class="reference internal" href="#architecture" id="id11">Architecture</a></li>
<li><a class="reference internal" href="#implementation" id="id12">Implementation</a><ul>
<li><a class="reference internal" href="#error-clipping" id="id13">Error clipping</a></li>
<li><a class="reference internal" href="#getting-tensorflow-ops-tensors-by-name" id="id14">Getting TensorFlow ops/tensors by name</a></li>
<li><a class="reference internal" href="#measuring-gpu-memory-usage" id="id15">Measuring GPU memory usage</a></li>
<li><a class="reference internal" href="#gpu-memory-management" id="id16">GPU memory management</a></li>
<li><a class="reference internal" href="#api" id="id17">API</a></li>
</ul>
</li>
<li><a class="reference internal" href="#training" id="id18">Training</a><ul>
<li><a class="reference internal" href="#using-target-q-network-and-error-clipping" id="id19">Using target Q-network and error clipping</a></li>
<li><a class="reference internal" href="#increasing-the-size-of-minibatch" id="id20">Increasing the size of minibatch</a></li>
</ul>
</li>
<li><a class="reference internal" href="#result" id="id21">Result</a></li>
<li><a class="reference internal" href="#references" id="id22">References</a></li>
</ul>
</div>
<div class="section" id="introduction">
<h2>Introduction</h2>
<p>This is the second of a series of blog posts that will describe my project</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/chan-y-park/rl-atari">https://github.com/chan-y-park/rl-atari</a></li>
</ul>
<p>of implementing reinforcement learning of Atari games using <a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a> and <a class="reference external" href="https://gym.openai.com/">OpenAI Gym</a>. In <a class="reference external" href="./rl_atari_part_1.html">Part 1</a> of this series we focused on <a class="reference external" href="https://github.com/chan-y-park/rl-atari/tree/simple_dqn">simple_dqn</a>, which implements a simplest deep Q-network. Here we build a deeper Q-network with enhanced training by introducing target Q-network and error clipping,</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/chan-y-park/rl-atari/tree/dqn_nature">https://github.com/chan-y-park/rl-atari/tree/dqn_nature</a></li>
</ul>
<p>by closely following the construction in <a class="footnote-reference" href="#mnih15" id="id1">[2]</a>. As in <a class="reference external" href="./rl_atari_part_1.html">Part 1</a>, I will go into some implementation details I gathered while implementing the deep Q-network.</p>
</div>
<div class="section" id="architecture">
<h2>Architecture</h2>
<p>The architecture of the Q-network of <a class="footnote-reference" href="#mnih15" id="id2">[2]</a> is not much different from that of <a class="footnote-reference" href="#mnih13" id="id3">[1]</a>, it is just deeper and has more weights:</p>
<ul class="simple">
<li>First it takes a tensor of dimension <tt class="docutils literal">[84, 84, 4]</tt> as an input, which is a stack of four grayscale images preprocessed from the screen captured from the Atari emulator.</li>
<li>The next layer is a 2d convolution layer, with a filter of dimension <tt class="docutils literal">[height, width, in, out] = [8, 8, 4, 32]</tt> and a stride of <tt class="docutils literal">4</tt>, followed by a ReLU layer. The resulting tensor is of dimension <tt class="docutils literal">[20, 20, 32]</tt>.</li>
<li>Then there is another 2d convolution layer, with a filter of dimension <tt class="docutils literal">[4, 4, 32, 64]</tt> and a stride of <tt class="docutils literal">2</tt>, followed by a ReLU layer. The output of this layer is of dimension <tt class="docutils literal">[9, 9, 64]</tt>.</li>
<li>The last 2d convolution layer has a filter of dimension <tt class="docutils literal">[3, 3, 64, 64]</tt> and a stride of <tt class="docutils literal">1</tt>, followed by a ReLU layer. The output of this layer is of dimension <tt class="docutils literal">[7, 7, 64]</tt>.</li>
<li>The previous output is flattened and used as an input for the first fully connected layer followed by a ReLU layer, whose output is of dimension <tt class="docutils literal">[512]</tt>.</li>
<li>The last layer is another fully connected layer without any nonlinear unit, and the output has the same dimension as the number of actions.</li>
</ul>
<p>This is shown in the following TensorFlow graph obtained using Tensorboard.</p>
<img alt="Nature DQN Graph" src="./figures/rl_atari/dqn_nature_graph.png" />
<p>Then there is a target Q-network, which is a clone of the Q-network and is updated by copying the weights of the Q-network using <tt class="docutils literal">tf.assign</tt> ops under <tt class="docutils literal">update</tt> namespace. The training is again similar to that of <a class="footnote-reference" href="#mnih13" id="id4">[1]</a>, which is done using <tt class="docutils literal">tf.train.RMSPropOptimizer</tt> under <tt class="docutils literal">train</tt> namespace, except error clipping is used when calculating the loss.</p>
</div>
<div class="section" id="implementation">
<h2>Implementation</h2>
<div class="section" id="error-clipping">
<h3>Error clipping</h3>
<p>The error clipping of <a class="footnote-reference" href="#mnih15" id="id5">[2]</a> corresponds to using the following loss function:</p>
<img alt="error clipping plot" class="align-center" src="./figures/rl_atari/error_clipping.png" />
<p>An easy way to implement this loss function is using <tt class="docutils literal">tf.minimum</tt>:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariDQNAgent</span><span class="p">:</span>

    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">_build_train_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># ...</span>

        <span class="n">diffs</span> <span class="o">=</span> <span class="n">ys</span> <span class="o">-</span> <span class="n">Qs_of_action</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="p">[</span><span class="s1">&#39;clip_error&#39;</span><span class="p">]:</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Use error clipping...&#39;</span><span class="p">)</span>
            <span class="n">deltas</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">diffs</span><span class="p">),</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diffs</span><span class="p">),</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;clipped_deltas&#39;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># ...</span>
</pre></div>
</div>
<div class="section" id="getting-tensorflow-ops-tensors-by-name">
<h3>Getting TensorFlow ops/tensors by name</h3>
<p>Compared to <a class="reference external" href="https://github.com/chan-y-park/rl-atari/tree/simple_dqn">simple_dqn</a>, here we use TensorFlow's namespace more extensibly, both for clear separation among various functionalities and also for the ease of retrieving various ops and tensors defined in the TensorFlow graph without using arbitrary variables.</p>
<p>For that purpose, first we need to distingush ops and tensors in TensorFlow. A TensorFlow op, or operation, takes input tensors and returns output tensors. And the name of an output tensor is defined as <tt class="docutils literal">'operation name' + ':' + 'index in output tensor list'</tt>. For example, in the following snippet,</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;train&#39;</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">deltas</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
    <span class="c1"># ...</span>
</pre></div>
<p>A TensorFlow op <tt class="docutils literal">tf.reduce_mean</tt> is defined with name <tt class="docutils literal">train/loss</tt>, takes a tensor referenced by variable <tt class="docutils literal">deltas</tt>, and gives an output tensor named <tt class="docutils literal">train/loss:0</tt>. Ops and tensors can be retrieved using <tt class="docutils literal">tf.Graph.get_operation_by_name</tt> and <tt class="docutils literal">tf.Graph.get_tensor_by_name</tt>, respectively, as illustrated in the following snippet.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariDQNAgent</span><span class="p">:</span>

    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">_get_tf_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tf_graph</span><span class="o">.</span><span class="n">get_operation_by_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_tf_t</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tf_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">_optimize_Q</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="c1"># ...</span>

        <span class="n">fetches</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_tf_op</span><span class="p">(</span><span class="s1">&#39;train/minimize_loss&#39;</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_tf_t</span><span class="p">(</span><span class="s1">&#39;train/loss:0&#39;</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_tf_t</span><span class="p">(</span><span class="s1">&#39;summary/train/loss:0&#39;</span><span class="p">),</span>
        <span class="p">]</span>

        <span class="c1"># ...</span>
</pre></div>
</div>
<div class="section" id="measuring-gpu-memory-usage">
<h3>Measuring GPU memory usage</h3>
<p>In <a class="reference external" href="visualizing_convnet.html#gpu-memory-usage">Visualizing Convolutional Network - GPU memory usage</a>, we discussed how to obtain the GPU memory usage using a custom op. From TensorFlow 1.1, we have a TensorFlow op <tt class="docutils literal">tf.contrib.memory_stats.MaxBytesInUse()</tt> that can be used in similar way but without building a custom op. We can use the op in the following way:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="k">with</span> <span class="n">agent</span><span class="o">.</span><span class="n">_tf_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">memory_used</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">_tf_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">memory_stats</span><span class="o">.</span><span class="n">MaxBytesInUse</span><span class="p">()</span>
    <span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">memory_used</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">**</span> <span class="mi">2</span>
<span class="go">604.752685546875</span>
</pre></div>
<p>So it tells us that about 605 MB of GPU memory is being used. However, this does not count all the GPU memory used by the TensorFlow process, for example when compared to the output of <tt class="docutils literal"><span class="pre">nvidia-smi</span></tt> command line utility as shown in the following:</p>
<div class="highlight"><pre><span></span>$ nvidia-smi --query-compute-apps<span class="o">=</span>pid,process_name,used_memory --format<span class="o">=</span>csv,nounits
pid, process_name, used_gpu_memory <span class="o">[</span>MiB<span class="o">]</span>
<span class="m">18428</span>, /home/chan/venv/bin/python3.6, <span class="m">1243</span>
</pre></div>
<p>This is more than two times of the memory usage returned by <tt class="docutils literal">tf.contrib.memory_stats.MaxBytesInUse()</tt>, and this is what actually matters to run the process.</p>
<p>There are a few utilites like <a class="reference external" href="https://github.com/wookayin/gpustat">gpustat</a> that fully parses the output of <tt class="docutils literal"><span class="pre">nvidia-smi</span></tt>, but we just need to obtain a small amount of result and because usually Python's <tt class="docutils literal">subprocess</tt> calls are expensive, here we just implement a small code to get the used GPU memory by PID as defined in <tt class="docutils literal">get_used_gpu_memory</tt></p>
<div class="highlight"><pre><span></span><span class="n">NVIDIA_SMI_ARGS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;nvidia-smi&#39;</span><span class="p">,</span>
    <span class="s1">&#39;--query-compute-apps=pid,used_memory&#39;</span><span class="p">,</span>
    <span class="s1">&#39;--format=csv,noheader&#39;</span><span class="p">,</span>
<span class="p">]</span>

    <span class="k">class</span> <span class="nc">AtariDQNAgent</span><span class="p">:</span>
        <span class="c1"># ...</span>

        <span class="k">def</span> <span class="nf">get_used_gpu_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="c1"># ...</span>
            <span class="n">smi_outputs</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span>
                <span class="n">NVIDIA_SMI_ARGS</span><span class="p">,</span>
                <span class="n">universal_newlines</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
            <span class="c1"># ...</span>
</pre></div>
</div>
<div class="section" id="gpu-memory-management">
<h3>GPU memory management</h3>
<p>By default a TensorFlow process allocates as much GPU memory as possible to itself, which prevents another TensorFlow process from running at the same time. To stop a TensorFlow process from using all GPU memory, we can</p>
<ul class="simple">
<li>let TensorFlow allocates as small amount of memory as possible to a process and then let its GPU memory usage grow as the process runs, and</li>
<li>specify the maximum amount of GPU memory the process can use</li>
</ul>
<p>by using options in <tt class="docutils literal">tf.ConfigProto()</tt> in the following way:</p>
<div class="highlight"><pre><span></span><span class="n">tf_config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">()</span>
<span class="n">tf_config</span><span class="o">.</span><span class="n">gpu_options</span><span class="o">.</span><span class="n">allow_growth</span> <span class="o">=</span> <span class="n">gpu_memory_allow_growth</span>
<span class="k">if</span> <span class="n">gpu_memory_fraction</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">tf_config</span><span class="o">.</span><span class="n">gpu_options</span><span class="o">.</span><span class="n">per_process_gpu_memory_fraction</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">gpu_memory_fraction</span>
    <span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_tf_session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="api">
<h3>API</h3>
<p>To train an agent, run</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dqn</span> <span class="kn">import</span> <span class="n">AtariDQNAgent</span>
<span class="kn">from</span> <span class="nn">configs</span> <span class="kn">import</span> <span class="n">dqn_nature_configuration</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">AtariDQNAgent</span><span class="p">(</span>
    <span class="n">game_name</span><span class="o">=</span><span class="s1">&#39;Breakout-v0&#39;</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">dqn_nature_configuration</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">stats</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">max_num_of_steps</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="mi">6</span><span class="p">),</span>
    <span class="n">run_name</span><span class="o">=</span><span class="s1">&#39;nature_config&#39;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
<p>which saves checkpoints as <tt class="docutils literal">checkpoints\nature_config*</tt>, logs for TensorBoard at <tt class="docutils literal">logs\nature_config\</tt>, and the configuration as <tt class="docutils literal">runs\nature_config</tt> JSON file. Then we can resume the training using the last checkpoint as</p>
<div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">AtariDQNAgent</span><span class="p">(</span>
    <span class="n">game_name</span><span class="o">=</span><span class="s1">&#39;Breakout-v0&#39;</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">dqn_nature_configuration</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">stats</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">max_num_of_steps</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="mi">6</span><span class="p">),</span>
    <span class="n">run_name</span><span class="o">=</span><span class="s1">&#39;nature_config_2&#39;</span><span class="p">,</span>
    <span class="n">save_path</span><span class="o">=</span><span class="s1">&#39;checkpoints/nature_config-1000000&#39;</span>
<span class="p">)</span>
</pre></div>
<p>although it only imports weights for the Q-networks so it takes some time to fill up the replay memory. After the training, we can let the agent play games, with higher <span class="math">\(\epsilon\)</span> (0.05 when playing vs. minimum 0.1 when training) of the <span class="math">\(\epsilon\)</span>-greedy policy.</p>
<div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">AtariDQNAgent</span><span class="p">(</span>
    <span class="n">game_name</span><span class="o">=</span><span class="s1">&#39;Breakout-v0&#39;</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">dqn_nature_configuration</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">stats</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">play</span><span class="p">(</span>
    <span class="n">max_num_of_episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">save_path</span><span class="o">=</span><span class="s1">&#39;checkpoints/nature_config-2000000&#39;</span>
<span class="p">)</span>
</pre></div>
<p>after which it outputs a GIF animation as <tt class="docutils literal"><span class="pre">play-Breakout-v0.gif</span></tt>.</p>
</div>
</div>
<div class="section" id="training">
<h2>Training</h2>
<p>Here we perform a few experiments to see how target Q-network and error clipping affect training of the Q-network.</p>
<div class="section" id="using-target-q-network-and-error-clipping">
<h3>Using target Q-network and error clipping</h3>
<p>We first compare the training of the NIPS architecture with and without target Q-network.</p>
<img alt="NIPS DQN legend" class="align-left" src="./figures/rl_atari/nips_target_Q_legend.png" />
<img alt="NIPS DQN Q" class="align-center" src="./figures/rl_atari/nips_target_Q_Q.png" />
<img alt="NIPS DQN legend" class="align-left" src="./figures/rl_atari/nips_target_Q_legend.png" />
<img alt="NIPS DQN reward" class="align-center" src="./figures/rl_atari/nips_target_Q_reward.png" />
<p>We can see that introducing a target Q-network stabilizes the training from the plot of average Q values, where we have far less oscillation of Q values when we use a target Q-network, espeically during the initial stage of the training. However, we cannot see any gain both from the Q value plot and the reward value plot, and this is probably because the NIPS architecture is not deep enough to benefit from the introduction of a Q-network during training.</p>
<p>Now let's see the result of a similar experiment, this time the training of the Nature architecture with target Q-network but without error clipping, with error clipping but without target Q-network, and without both of them.</p>
<img alt="Nature DQN experiment 1 legend" class="align-left" src="./figures/rl_atari/nature_target_Q_error_clipping_legend.png" />
<img alt="Nature DQN experiment 1 DQN Q" class="align-center" src="./figures/rl_atari/nature_target_Q_error_clipping_Q.png" />
<img alt="Nature DQN experiment 1 legend" class="align-left" src="./figures/rl_atari/nature_target_Q_error_clipping_legend.png" />
<img alt="Nature DQN experiment 1 reward" class="align-center" src="./figures/rl_atari/nature_target_Q_error_clipping_reward.png" />
<p>Now we see that the introduction of target Q-networks makes or breaks the training. Without using target Q-network, the deeper Q-network is not trained properly. Introduction of error clipping also makes a big difference in terms of average Q values, as it rapidly stabilizes the initial divergence in Q estimation, probably thanks to using a linear loss when the value of loss is larger than 1 and therefore having less divergent value compared to the squared error. But its effect is not as dramatic as using a target Q-network, which is more evident when we see the average reward plot and also the following plot where the result of without both target Q-network and error clipping is removed and the result of both of them being used, i.e. the exact training algorithm used in <a class="footnote-reference" href="#mnih15" id="id6">[2]</a>, is plotted.</p>
<img alt="Nature DQN experiment 2 legend" class="align-left" src="./figures/rl_atari/nature_exp_2_legend.png" />
<img alt="Nature DQN experiment 2 DQN Q" class="align-center" src="./figures/rl_atari/nature_exp_2_Q.png" />
<img alt="Nature DQN experiment 2 legend" class="align-left" src="./figures/rl_atari/nature_exp_2_legend.png" />
<img alt="Nature DQN experiment 2 reward" class="align-center" src="./figures/rl_atari/nature_exp_2_reward.png" />
<p>We can see that using a target Q-network alone is quite enough without using error clipping, as we see little difference between using it or not as long as we use a target Q-network. This makes me guess that the authors of <a class="footnote-reference" href="#mnih15" id="id7">[2]</a> probably first tried to stabilize the wild behaviour of the Nature architecture during training using error clipping only, and after that they may have discovered that using a target Q-network is more beneficial but left error clipping there to be safe.</p>
</div>
<div class="section" id="increasing-the-size-of-minibatch">
<h3>Increasing the size of minibatch</h3>
<p>When training the Nature deep Q-network, the GPU usage was below 40%, and I wondered if GPU can be fully utilized by using a larger minibatch, as well as improving the training in the sense that it effectively undergoes more weight updates.</p>
<p>Of course the fundamental reason for the less-than-optimal usage of the GPU is because it takes quite much time to sample and fill up a minibatch from the replay memory, as this is done on the CPU side. If I had a GPU with more memory than I can try pushing all the data to GPU but that was not an option for me.</p>
<p>Then the second best solution would be maintaining a separate thread for a queue to which the CPU puts in minibatches and the GPU consumes it as fast as possible. But this requires quite a work so I postponed it to a future project.</p>
<p>Compared to the above, changing the size of the minibatch is very simple, so I tried doubling the size from 32 to 64, whose result is shown in the following plot.</p>
<img alt="Nature DQN legend" class="align-left" src="./figures/rl_atari/nature_training_legend.png" />
<img alt="Nature DQN Q" class="align-center" src="./figures/rl_atari/nature_training_Q.png" />
<img alt="Nature DQN legend" class="align-left" src="./figures/rl_atari/nature_training_legend.png" />
<img alt="Nature DQN reward" class="align-center" src="./figures/rl_atari/nature_training_reward.png" />
<p>As we can see, doubling the minibatch size did not help the training. In retrospect, even if we increase the size of a minibatch, the number of weight updates will be the same, because we sum all the erros to get a scalar loss, and that may be the reason why changing the size of a minibatch does not give a noticeable difference in training. It would be interesing to do more experiment on this matter, but I think introducing a training queue will be far more productive effort, so I won't pursue the tuning of the minibatch size.</p>
<p>The plot also shows the result after 5M weight updates. Compared to the steep increase of reward values between 1M and 2M steps, the reward plot somewhat plateaus after 2M. The average Q-value plot shows the training is still linearly progressing so the training itself is going on smoothly, but I will first revisit the code to improve the efficiency of the training and then try training it for 10M and 50M updates to compare the results with those of <a class="footnote-reference" href="#mnih15" id="id8">[2]</a> on equal footing.</p>
</div>
</div>
<div class="section" id="result">
<h2>Result</h2>
<p>This is the result of the training after 5M weight updates using the configuration of <a class="footnote-reference" href="#mnih15" id="id9">[2]</a>. Its play is much better when compared to the result of <a class="reference external" href="./rl_atari_part_1.html">Part 1</a>.</p>
<img alt="Nature DQN play" class="align-center" src="./figures/rl_atari/play-5M.gif" />
</div>
<div class="section" id="references">
<h2>References</h2>
<table class="docutils footnote" frame="void" id="mnih13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#id3">1</a>, <a class="fn-backref" href="#id4">2</a>)</em> Playing Atari with Deep Reinforcement Learning, <a class="reference external" href="https://arxiv.org/abs/1312.5602">https://arxiv.org/abs/1312.5602</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="mnih15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id2">2</a>, <a class="fn-backref" href="#id5">3</a>, <a class="fn-backref" href="#id6">4</a>, <a class="fn-backref" href="#id7">5</a>, <a class="fn-backref" href="#id8">6</a>, <a class="fn-backref" href="#id9">7</a>)</em> Human-level control through deep reinforcement learning, <a class="reference external" href="https://doi.org/10.1038/nature14236">https://doi.org/10.1038/nature14236</a></td></tr>
</tbody>
</table>
</div>
<script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            var disqus_shortname = 'chan-y-park'; // required: replace example with your forum shortname

            var disqus_identifier = 'rl_atari_part_2';
            var disqus_url = 'http://chan-y-park.github.io/blog/rl_atari_part_2.html';

            var disqus_config = function () {
                this.page.url = disqus_url;
                this.page.identifier = disqus_identifier;
                this.language = "en";
            };

            (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>

<section class="well well-sm">
    <ul class="list-group list-group-flush">
            <li class="list-group-item"><h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Follow</span></h4>
              <ul class="list-group" id="social">
                <li class="list-group-item"><a href="http://github.com/chan-y-park"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
                <li class="list-group-item"><a href="http://www.linkedin.com/in/chan-youn-park-51a13b63"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
                <li class="list-group-item"><a href="http://twitter.com/chan_y_park"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
                <li class="list-group-item"><a href="http://www.facebook.com/chan.y.park.5"><i class="fa fa-facebook-square fa-lg"></i> Facebook</a></li>
              </ul>
            </li>





    </ul>
</section>            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2017 Chan Y. Park
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="./theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="./theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="./theme/js/respond.min.js"></script>

    <!-- Disqus -->
    <script type="text/javascript">
    /*
        var disqus_shortname = 'chan-y-park'; // required: replace example with your forum shortname

        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
    */
    </script>
    <!-- End Disqus Code -->

        <script type="text/javascript">var addthis_config = {"data_track_addressbar": true};</script>
    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=github blog"></script>
</body>
</html>